<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>Converting large csv&#39;s to nested data structure using apache spark | Sudev Ambadi</title>

<meta property='og:title' content='Converting large csv&#39;s to nested data structure using apache spark - Sudev Ambadi'>
<meta property='og:description' content='What is Apache Spark ? Apache Spark brings fast, in-memory data processing to Hadoop. Elegant and expressive development APIs in Scala, Java, and Python allow data workers to efficiently execute streaming, machine learning or SQL workloads for fast iterative access to datasets.
Quick start guide
Problem Statement / Task To read lot of really big csv&rsquo;s (~GBs) from Hadoop HDFS, clean, convert them to nested data structure and update it to MongoDB using Apache Spark.'>
<meta property='og:url' content='https://example.com/post/2015-07-18-mongospark/'>
<meta property='og:site_name' content='Sudev Ambadi'>
<meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/392ea4e3015cf654cbd3974ac2c29e64?s=256'><meta property='article:publisher' content='sudev.ambadi'><meta property='article:section' content='Post'><meta property='article:tag' content='Apache Spark'><meta property='article:tag' content='MongoDB'><meta property='article:tag' content='large csv'><meta property='article:tag' content='data cleaning'><meta property='article:tag' content='reducebykey'><meta property='article:published_time' content='2015-07-18T00:00:00Z'/><meta property='article:modified_time' content='2015-07-18T00:00:00Z'/><meta name='twitter:card' content='summary'><meta name='twitter:site' content='@sudev'><meta name='twitter:creator' content='@sudev'>
<link rel="stylesheet" href="https://example.com/css/style.css"/><link rel='stylesheet' href='/css/custom.css'></head>
<body>

<section class="section">
  <div class="container">
    <nav class="nav">
      <div class="nav-left">
        <a class="nav-item" href="https://example.com"><h1 class="title is-4">Sudev Ambadi</h1></a>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile"><a class="level-item" href='mailto:sudevdev@gmail.com' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
    <polyline points="22,6 12,13 2,6"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://github.com/sudev' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://instagram.com/sudev_ambadi' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <rect x="2" y="2" width="20" height="20" rx="5" ry="5"/>
    <path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"/>
    <line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://twitter.com/sudev' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="subtitle is-6 is-pulled-right">
      
      
<a class="subtitle is-6" href="/tags/apache-spark">#Apache Spark</a>



  
  | <a class="subtitle is-6" href="/tags/mongodb">#MongoDB</a>
  
  | <a class="subtitle is-6" href="/tags/large-csv">#large csv</a>
  
  | <a class="subtitle is-6" href="/tags/data-cleaning">#data cleaning</a>
  
  | <a class="subtitle is-6" href="/tags/reducebykey">#reducebykey</a>
  

      
    </div>
    <h2 class="subtitle is-6">July 18, 2015</h2>
    <h1 class="title">Converting large csv&#39;s to nested data structure using apache spark</h1>
    
    <div class="content">
      

<h5 id="what-is-apache-spark">What is Apache Spark ?</h5>

<p><a href="http://spark.apache.org/" title="Apache Spark homepage" target="_blank">Apache Spark</a> brings fast, in-memory data processing to Hadoop. Elegant and expressive development APIs in Scala, Java, and Python allow data workers to efficiently execute streaming, machine learning or SQL workloads for fast iterative access to datasets.<br />
<a href="http://spark.apache.org/docs/latest/quick-start.html" title="Spark quick start" target="_blank">Quick start guide</a></p>

<h5 id="problem-statement-task">Problem Statement / Task</h5>

<p>To read lot of really big csv&rsquo;s (~GBs) from Hadoop HDFS, clean, convert them to nested data structure and update it to MongoDB using Apache Spark.
<br />
Recently I was assigned to create a Mongo collection with some select financial values by reading csv&rsquo;s containing income statements, balance sheets &hellip; junk data.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">CompanyID,USDAmount,YearEnding,Label,BalSubCategoryName,BalSubCategoryCode,LabelOrderId,SubCategoryOrder
1235,14737.251,31-01-2010,Non-Current Assets,Intangible assets,Non_Curr_Asset_Sub_03,12,152
1235,0,31-01-2009,Non-Current Assets,Intangible assets,Non_Curr_Asset_Sub_13,13,155
1235,10733.189,31-01-2011,Non-Current Assets,Intangible assets,Non_Curr_Asset_Sub_10,11,125</code></pre></div>

<p>Shown above is sample csv, I had to convert them into schema as shown below and update them to MongoDB. Consider a scenario where each csv is about ~ 1 GB and you have hundreds of them.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#f92672">&#34;1235&#34;</span>: {
        <span style="color:#f92672">&#34;2009&#34;</span>: {
            <span style="color:#f92672">&#34;Non-CurrentAssets&#34;</span>: <span style="color:#ae81ff">0</span>
        },
        <span style="color:#f92672">&#34;2010&#34;</span>: {
            <span style="color:#f92672">&#34;Non-CurrentAssets&#34;</span>: <span style="color:#ae81ff">14737</span>
        },
        <span style="color:#f92672">&#34;2011&#34;</span>: {
            <span style="color:#f92672">&#34;Non-CurrentAssets&#34;</span>: <span style="color:#ae81ff">10733.189</span>
        }
    }
}</code></pre></div>

<h5 id="approach">Approach</h5>

<ol>
<li><em>Data Cleaning</em> - Read multiple types of csv&rsquo;s and convert all of them into tuples of structure <code>(CompanyName, Map&lt;Year, Map&lt;TagName, Value&gt;&gt;&gt;)</code>.</li>
<li><em>Union all created RDDs</em> - Join all the cleaned csv rdd into one.</li>
<li><em>Reduce</em> - Reduce all tuples related to a company into single tuple considering companyName as the key.</li>
<li><em>Update MongoDB</em> - Update MongoDB with reduced tuples.</li>
</ol>

<h4 id="data-cleaning">Data Cleaning</h4>

<p>The order of fields in the csv dump differs according to the type of csv, so I had to write a generic function wherein we can specify the position of required fields. So let&rsquo;s call this function on both income-statement.csv and balance-sheet.csv and to create two cleaned rdd datasets  <code>balanceSheetRdd</code> and <code>incomeStatemntRdd</code> and later join them into one <code>masterRdd</code>.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">// Function definition
JavaPairRDD&lt;String, Map&lt;String, Map&lt;String, String&gt;&gt;&gt; dataclean(
			JavaSparkContext sc,                      // Spark Context 
			String filepath,                         // path to file in Hadoop
			final Set&lt;String&gt; filterTag,             // Required financial tags 
			final int pos_tag,  final int pos_cname, // Position  
			final int pos_date, final int pos_value)</code></pre></div>

<p>The <a href="https://github.com/databricks/spark-csv" target="_blank">spark-csv</a> plug-in can be used to read csv&rsquo;s into a dataframe rdd, the plug-in is recommended over <code>map(line.split(&quot;,&quot;))</code> for its ability to handle quotes and malformed entries.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">DataFrame df = sqlContext.read()
				.format(&#34;com.databricks.spark.csv&#34;)
				.option(&#34;header&#34;, &#34;true&#34;).load(filepath);
// spark-csv outputs dataframe to iterate line by line
// we will have to convert it to RDD of Rows
JavaRDD&lt;Row&gt; rowRdd = df.javaRDD();</code></pre></div>

<p>Create two Java sets with required tags that we are planning to extract from the csv.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">// Income Statement required tags 
final Set&lt;String&gt; filterTagsIS = new java.util.HashSet&lt;String&gt;();
filterTagsIS.add(&#34;Revenue&#34;);
filterTagsIS.add(&#34;Cost of sales&#34;);
// Balance Statement required tags
final Set&lt;String&gt; filterTagsBS = new java.util.HashSet&lt;String&gt;();
filterTagsBS.add(&#34;Total Non Current Assets&#34;);
filterTagsBS.add(&#34;Total Assets&#34;);</code></pre></div>

<p>Filter out the unwanted tags using Sparks filter action.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">filteredRdd = rowRdd.filter(new Function&lt;Row, Boolean&gt;() {  
@Override
public Boolean call(Row r) throws Exception {
	return filterTag.contains(r.getString(pos_tag));
}
})</code></pre></div>

<p>From the filtered rdd create a new PairRdd (tuple) of the form <code>(CompanyName, Map&lt;Year, Map&lt;TagName, Value&gt;&gt;&gt;)</code> using Spark mapToPair action.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">cleanedRdd = filteredRdd.mapToPair( 
new PairFunction&lt;Row, String, Map&lt;String, Map&lt;String, String&gt;&gt;&gt;() {
	@Override
	public Tuple2&lt;String, Map&lt;String, Map&lt;String, String&gt;&gt;&gt; call(
			Row r) {
		Map&lt;String, String&gt; m1 = new HashMap&lt;String, String&gt;();
		Map&lt;String, Map&lt;String, String&gt;&gt; m2 = new HashMap&lt;String, Map&lt;String, String&gt;&gt;();
		String label = r.getString(pos_tag);
		// create a map of the form { Tag : value }
		m1.put(label, r.getString(pos_value));
		String year = r.getString(pos_date).substring(
				r.getString(pos_date).length() - 4);
		// create a map of the form 
		// { year :  { tag : value }   }
		m2.put(year, m1);
		return new Tuple2&lt;String, Map&lt;String, Map&lt;String, String&gt;&gt;&gt;(r.getString(pos_cname), m2);
	}
}
);</code></pre></div>

<p>Now we have cleaned the entire csv file contents into desirable format. Here I have arranged the filter and mapToPair actions into <a href="https://github.com/sudev/sparkMongo/blob/master/src/main/java/mongoDump/DataCleaning.java" target="_blank">data cleaning class</a>.</p>

<h4 id="union">Union</h4>

<p>Assuming we have created two rdd&rsquo;s balanceSheetRdd and incomeStatemntRdd using above method. Make a master rdd using spark union transformation. From here on masterRdd will be instead of balanceSheetRdd and IncomeStatementRdd.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">masterRdd = balanceSheetRdd.union(incomeStatemntRdd)</code></pre></div>

<h4 id="reduce">Reduce</h4>

<p>Reduce the master rdd with companyName as the key. Idea is to aggregate all financial details related to a company aggregated year wise. Calling <code>reduceByKey()</code> on masterRdd will produce iterable list using companyName as key but we need to do more here, we have to aggregate them according to year. We can do this by writing a custom class implementing Function2.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">reducedRdd = masterRdd.raduceByKey(new reduceMaps())</code></pre></div>

<p>Class reduceMaps, takes two tuples with same comapnyName and then reduces it by correctly grouping the tags by year.
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">final class reduceMaps
		implements
		Function2&lt;Map&lt;String, Map&lt;String, String&gt;&gt;, Map&lt;String, Map&lt;String, String&gt;&gt;, Map&lt;String, Map&lt;String, String&gt;&gt;&gt; {
	public Map&lt;String, Map&lt;String, String&gt;&gt; call(
			Map&lt;String, Map&lt;String, String&gt;&gt; map0,
			Map&lt;String, Map&lt;String, String&gt;&gt; map1) throws Exception {
		Set&lt;Entry&lt;String, Map&lt;String, String&gt;&gt;&gt; emap0 = map0.entrySet();
		// Iterate on map0 and update map1
		for (Entry&lt;String, Map&lt;String, String&gt;&gt; entry : emap0) {
			Map&lt;String, String&gt; val = map1.get(entry.getKey());
			if (val == null) {
				map1.put(entry.getKey(), entry.getValue());
			} else {
				// If present, take union of inner map and replace
				val.putAll(entry.getValue());
				map1.put(entry.getKey(), val);
			}
		}
		return map1;
	}
}</code></pre></div></p>

<h4 id="updating-mongodb">Updating MongoDB</h4>

<p>To update mongoDB using Spark use <a href="https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage" title="Hadoop Spark Mongo connector wiki" target="_blank">mongo-hadoop connector</a>. Before saving the rdd covert them into pairRdds of the type <code>JavaPairRDD&lt;Object, BSONObject&gt;</code>.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">mongoRdd = reducedRdd.mapToPair( new basicDBMongo())</code></pre></div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">final class basicDBMongo implements PairFunction&lt;Tuple2&lt;String, Map&lt;String, Map&lt;String, String&gt;&gt;&gt;, Object, BSONObject&gt; {
	public Tuple2&lt;Object, BSONObject&gt; call(
			Tuple2&lt;String, Map&lt;String, Map&lt;String, String&gt;&gt;&gt; companyTuple)
			throws Exception {
		BasicBSONObject report = new BasicBSONObject();
		// Create a BSON of form { companyName : financeDetails } 
		report.put(companyTuple._1(), companyTuple._2());
		return new Tuple2&lt;Object, BSONObject&gt;(null, report);
	}
}</code></pre></div>

<p>Updating mongoDB</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-csv" data-lang="csv">// Configurations for Mongo Hadoop Connector
String mongouri = &#34;mongo:url/db/collectioName&#34;
org.apache.hadoop.conf.Configuration midbconf = new org.apache.hadoop.conf.Configuration();
midbconf.set(&#34;mongo.output.format&#34;,
		&#34;com.mongodb.hadoop.MongoOutputFormat&#34;);
midbconf.set(&#34;mongo.output.uri&#34;, mongouri);
// Writing the rdd to Mongo
mongordd.saveAsNewAPIHadoopFile(&#34;file:///notapplicable&#34;, Object.class,
				Object.class, MongoOutputFormat.class, midbconf);</code></pre></div>

<p>Actually we did quiet a lot of things here. This is how the DAG looks for this job.</p>

<p><br /></p>

<figure class="one">
    <img src="/images/stages.PNG">
    <figcaption>DAG for the job, helps in uderstanding and improving the whole process.</figcaption>
</figure>

<p><br /></p>

<p>Previously I had attempted to do this filtering and mapping jobs using dataframes, but the solution was not great. I like this program for the fact that I&rsquo;m not collecting anything from rdds into driver anywhere and hence this should run distributed at each stage. I ran and tested this application on a Spark Standalone Cluster on HDP Stack with 4 nodes.</p>

<p><br /></p>

<figure class="one">
    <img src="/images/active.PNG">
    <figcaption>The workload distributed evenly in cluster. Apache Spark :)</figcaption>
</figure>

<p><br /></p>

<p>Let me know your thoughts, please do comment. The entire <a href="https://github.com/sudev/sparkMongo" target="_blank">code is available in github</a>, this post intends to explain the same.</p>

    </div>
    
  </div>
</section>

<section class="section">
  <div class="container">
    <aside><div id="disqus_thread"></div></aside>
    <script type="text/javascript">
      var disqus_shortname = 'sudevgithub';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p>&copy; 2017 Sudev Ambadi. <br /> Unless noted otherwise, all content is available under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. </a> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/80x15.png" /></a></p>
    
      <p>Powered by <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/ribice/kiss">Kiss</a>.</p>
    
  </div>
</section>


<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-39443594-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


</body>
</html>
